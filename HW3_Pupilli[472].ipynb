{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to Luke's 472 Assignment 3, Regression Machine Learning.\n",
        "This is my attempt at training a machine learning model to predict prices of houses given a specific dataset."
      ],
      "metadata": {
        "id": "O8OyevkcMCfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps 1-3"
      ],
      "metadata": {
        "id": "pGcPdBvNMWYy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlEFapjo8PI0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Step 1: Load file\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "file = pd.read_csv('/content/drive/My Drive/data/area_house_data.csv')\n",
        "file.head()\n",
        "\n",
        "#Step 2: Explore data\n",
        "file.info()\n",
        "file.describe()\n",
        "\n",
        "#A histogram for each numerical attribute\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "file.hist(bins=50, figsize=(20,15))\n",
        "plt.show()\n",
        "\n",
        "#Step 3, cleanup data, dropping 'date' and 'id'\n",
        "file = file.drop('date', axis=1)\n",
        "file = file.drop('id', axis=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Interrelationship analysis and the relationship between each of the feature variables and the target variable (price)."
      ],
      "metadata": {
        "id": "1OZnJK44MawY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: Plot scatter plots for feature variables vs price\n",
        "feature_variables = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront',\n",
        "                     'view', 'condition', 'yr_renovated', 'grade',  'sqft_basement', 'yr_built',\n",
        "                     'zipcode', 'lat','long', 'sqft_living15']\n",
        "\n",
        "for feature in feature_variables:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(file[feature], file['price'], alpha=0.5)\n",
        "    plt.title(f'{feature.capitalize()} vs Price')\n",
        "    plt.xlabel(feature.capitalize())\n",
        "    plt.ylabel('Price')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "iDWjRKbXHIwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Perform Feature engineering inspired from the above visuals"
      ],
      "metadata": {
        "id": "dRUkL8SLMo3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Step 5: Drop features that are not useful (diffrentiated with a grey background) and creating new feature variable\n",
        "feature_variables_revised = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
        "                     'condition', 'grade',  'sqft_basement', 'zipcode']\n",
        "\n",
        "for feature in feature_variables_revised:\n",
        "    plt.figure(figsize=(8, 6), facecolor='lightgrey')\n",
        "    plt.scatter(file[feature], file['price'], alpha=0.5)\n",
        "    plt.title(f'{feature.capitalize()} vs Price')\n",
        "    plt.xlabel(feature.capitalize())\n",
        "    plt.ylabel('Price')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8P0bJrEcDNUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps 6-8, partition datasets in to training set and testing set.\n",
        "\n",
        "\n",
        "Then partition respective training/testing sets into features set/ target set(price)\n"
      ],
      "metadata": {
        "id": "L_rgeYM2Ab7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#create featured set\n",
        "features_set = file.drop(columns=[col for col in file.columns if col not in feature_variables_revised])\n",
        "\n",
        "# Separarte the features and target variables before the split\n",
        "\n",
        "print(\"Featured set: \")\n",
        "print(features_set.columns)\n",
        "\n",
        "\n",
        "target_var_set = file['price']\n",
        "\n",
        "features_train, features_test, target_train, target_test = train_test_split(features_set, target_var_set, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Length of features_train :\", len(features_train))\n",
        "print(\"Length of features_test :\", len(features_test))\n",
        "print(\"Length of target_train :\", len(target_train))\n",
        "print(\"Length of target_test :\", len(target_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rQxjw0Q5Aku-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82aad71e-ca8f-494d-f240-805de52e30c9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Featured set: \n",
            "Index(['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'condition',\n",
            "       'grade', 'sqft_basement', 'zipcode'],\n",
            "      dtype='object')\n",
            "Length of features_train : 17290\n",
            "Length of features_test : 4323\n",
            "Length of target_train : 17290\n",
            "Length of target_test : 4323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reasoning for splitting training and testing data 80%-20%?\n",
        "\n",
        "The 80-20 split strikes a balance between having enough data for training and enough data for testing. It also avoids allocating too little data to either set, which could lead to overfitting (training set too small) or unreliable evaluation (testing set too small)."
      ],
      "metadata": {
        "id": "Y48bZ2jZLtxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9, Train a linear regression model using the training dataset\n"
      ],
      "metadata": {
        "id": "gAMuGj28XDGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(features_train, target_train)\n",
        "\n",
        "file_training_prediction = lin_reg.predict(features_train)\n",
        "features_column = features_train.columns\n",
        "coefficients = lin_reg.coef_\n",
        "print(coefficients)"
      ],
      "metadata": {
        "id": "DELpXoBOXD66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10, Metrics on training data\n"
      ],
      "metadata": {
        "id": "X9nT0fOOKYz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MSE\n",
        "mean_squared_error_training_set = mean_squared_error(target_train, file_training_prediction)\n",
        "print(\"Mean Squared Error for Training Set: \",mean_squared_error_training_set)\n",
        "\n",
        "#RMSE\n",
        "RootMSE = np.sqrt(mean_squared_error_training_set)\n",
        "print(\"RMSE for training set:\", RootMSE)\n",
        "\n",
        "#R2\n",
        "r2_train = r2_score(target_train, file_training_prediction)\n",
        "print(\"r2 value: \", r2_train)"
      ],
      "metadata": {
        "id": "XSHgU87ZILGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 11, Testing trained model on test set\n",
        "Step 12, Metrics"
      ],
      "metadata": {
        "id": "fLN9U5YXKsTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction = lin_reg.predict(features_test)\n",
        "\n",
        "mse_testing_set = mean_squared_error(target_test, test_prediction)\n",
        "rmse_testing_set = np.sqrt(mse_testing_set)\n",
        "r2 = r2_score(target_test, test_prediction)\n",
        "\n",
        "print(\" Test RMSE Score:  \", rmse_testing_set)\n",
        "print(\"Test R2 Score: \", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyV-vf6RLp2O",
        "outputId": "19a9c94a-499f-44de-c583-88190e0bad2d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Test RMSE Score:   254064.88459400582\n",
            "Test R2 Score:  0.5730230892037547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. The testing set metrics showed a slight increase in accuracy from the training data. This tells me the the features set was slightly effective in training the model."
      ],
      "metadata": {
        "id": "A5cD-smlK75y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "14. It I were to do this again with another iteration, I would probably research most effective feature engineering techniques for improving machine learning models. This would likely be a better combination of combined variables, dropped inefficient data, and more that I haven't seen yet."
      ],
      "metadata": {
        "id": "7KBrtPsPLg_n"
      }
    }
  ]
}